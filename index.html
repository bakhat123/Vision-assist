<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web Vision AI Real-Time</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body { font-family: 'Inter', sans-serif; }
        #video-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin: 0 auto;
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 
                        0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }
        #webcam-video {
            display: block;
            width: 100%;
            height: auto;
            transform: scaleX(-1);
        }
        #capture-canvas {
            display: none;
        }
        #overlay-canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
        }
        #loading-overlay {
            position: absolute;
            inset: 0;
            background-color: rgba(17,17,17,0.75);
            display: flex;
            align-items: center;
            justify-content: center;
            display: none;
        }
    </style>
</head>
<body class="bg-gray-900 text-white min-h-screen p-4 flex flex-col items-center">

    <h1 class="text-3xl font-extrabold mb-6 text-teal-400">Web Vision AI Real-Time</h1>

    <div id="video-container" class="mb-6">
        <video id="webcam-video" autoplay playsinline class="w-full rounded-xl"></video>
        <canvas id="capture-canvas"></canvas>
        <canvas id="overlay-canvas"></canvas>
        <div id="loading-overlay">
            <svg class="animate-spin -ml-1 mr-3 h-8 w-8 text-white" 
                 xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                <circle class="opacity-25" cx="12" cy="12" r="10" 
                        stroke="currentColor" stroke-width="4"></circle>
                <path class="opacity-75" fill="currentColor" 
                      d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 
                      5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 
                      3 7.938l3-2.647z"></path>
            </svg>
            <span class="text-xl font-medium">Analyzing...</span>
        </div>
    </div>

    <div id="status-box" 
         class="w-full max-w-lg bg-gray-800 p-4 rounded-lg text-center shadow-lg transition duration-300">
        <p id="status-message" class="text-lg text-gray-300 font-medium">
            Initializing camera...
        </p>
    </div>

    <script>
        const video = document.getElementById('webcam-video');
        const canvas = document.getElementById('capture-canvas');
        const overlay = document.getElementById('overlay-canvas');
        const octx = overlay.getContext('2d');
        const statusMessage = document.getElementById('status-message');
        const loadingOverlay = document.getElementById('loading-overlay');
        const synth = window.speechSynthesis;

        const SERVER_URL = "https://10.7.33.86:8443/analyze-frame"; // replace with your backend
        let stream = null;
        let isProcessing = false;
        let lastAnnounced = "";
        const PROCESS_INTERVAL_MS = 1000; // analyze every 1 second

        function updateStatus(message) {
            statusMessage.textContent = message;
        }

        function speak(text) {
            if (synth.speaking) synth.cancel();
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 0.8;
            synth.speak(utterance);
        }

        function drawBoxes(objects) {
            octx.clearRect(0, 0, overlay.width, overlay.height);

            objects.forEach(obj => {
                const [x1, y1, x2, y2] = obj.bbox;
                const label = obj.label;
                const distance = obj.distance_cm ?? obj.distance_m ?? "N/A";

                octx.strokeStyle = 'red';
                octx.lineWidth = 2;
                octx.strokeRect(x1, y1, x2 - x1, y2 - y1);

                octx.fillStyle = 'red';
                octx.font = "16px Arial";
                octx.fillText(`${label} (${distance} cm)`, x1, y1 - 5);
            });
        }

        async function startCamera() {
            try {
                stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;

                video.onloadedmetadata = () => {
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;
                    overlay.width = video.videoWidth;
                    overlay.height = video.videoHeight;

                    updateStatus("Camera ready. Scanning continuously...");
                    continuousAnalyze();
                };
            } catch (err) {
                updateStatus("Error accessing camera. Please allow permissions.");
                console.error(err);
            }
        }

        async function continuousAnalyze() {
            if (!stream) return;

            setInterval(async () => {
                if (isProcessing) return;
                isProcessing = true;

                try {
                    const ctx = canvas.getContext('2d');
                    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                    const blob = await new Promise(resolve => canvas.toBlob(resolve, 'image/jpeg', 0.8));
                    const formData = new FormData();
                    formData.append('file', blob, 'frame.jpg');

                    const response = await fetch(SERVER_URL, { method: 'POST', body: formData });
                    if (!response.ok) throw new Error("Server error");

                    const data = await response.json();
                    const objects = data.objects || [];

                    // Draw bounding boxes
                    drawBoxes(objects);

                    let messages = [];

                    // Announce closest object
                    if (objects.length > 0) {
                        objects.sort((a, b) => (a.distance_cm ?? 9999) - (b.distance_cm ?? 9999));
                        const obj = objects[0]; // closest object
                        const announcement = obj.label;
                        const d = obj.distance_cm;
                        let phrase = "";
                        if (d !== undefined && d !== null) {
                            phrase = d < 100 ? `${Math.round(d)} cm ahead` : `${(d/100).toFixed(1)} meters ahead`;
                        }
                        const objMessage = `Detected ${announcement}, ${phrase}`;
                        messages.push(objMessage);

                        if (announcement !== lastAnnounced) {
                            speak(objMessage);
                            lastAnnounced = announcement;
                        }
                    }

                    // Process Texts
                    const texts = data.texts || [];
                    if (texts.length > 0) {
                        messages.push(`Text found: ${texts[0]}`);
                    }

                    // Process Faces
                    const faces = data.faces || [];
                    if (faces.length > 0) {
                        messages.push(`Detected ${faces.length} face${faces.length > 1 ? 's' : ''}`);
                    }

                    const finalMessage = messages.length === 0 
                        ? "No relevant objects, text, or faces detected."
                        : messages.join(". ");

                    updateStatus(finalMessage);

                } catch (err) {
                    console.error(err);
                    updateStatus("Error during analysis.");
                } finally {
                    isProcessing = false;
                }
            }, PROCESS_INTERVAL_MS);
        }

        window.onload = () => startCamera();
    </script>
</body>
</html>
